{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "54b847e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# install if needed:\n",
        "# !pip install python-docx lxml\n",
        "\n",
        "from docx import Document\n",
        "from lxml import etree\n",
        "import re, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "86aee58d",
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = Document('Original/Paiwan Ch2 Preprocessed.docx')\n",
        "raw_lines = [] # will hold list of all lines in word doc\n",
        "for para in doc.paragraphs:\n",
        "    for ln in para.text.split('\\n'): # split each paragraph on \\n\n",
        "        ln = ln.strip() # remove leading and trailing spaces/tabs\n",
        "        raw_lines.append(ln)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2b3e4cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_header(line):\n",
        "    # headers start with 3 digits followed by a space and do not contain tabs\n",
        "    return bool(re.match(r'^\\d{3}\\s+', line)) and '\\t' not in line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1577d07a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_sentence_start(line):\n",
        "    # 3 digits + tab\n",
        "    return bool(re.match(r'^\\d{3}\\t', line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "124508f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "dialect_map = {\n",
        "    # Eastern\n",
        "    \"Tjauvałi\":   \"Eastern\",\n",
        "    \"Tjavuałi\":   \"Eastern\",\n",
        "    \"Patjavał\":   \"Eastern\",\n",
        "    # Southern\n",
        "    \"Qatsiłay\":     \"Southern\",\n",
        "    \"Tjakuvukuvuł\": \"Southern\",\n",
        "    \"Łaleklek\":     \"Southern\",\n",
        "    # Central\n",
        "    \"Kułałau\":    \"Central\",\n",
        "    \"Tjałakavus\": \"Central\",\n",
        "    \"Kaviangan\":  \"Central\",\n",
        "    # Northern\n",
        "    \"Kapayuanan\": \"Northern\",\n",
        "    \"Vałulu\":     \"Northern\",\n",
        "    \"Tjukuvuł\":   \"Northern\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8360945f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "story 061, sentence 034\tamin. has 1 lines, dropping 1\n",
            "story 062, sentence 022\tamin. has 1 lines, dropping 1\n",
            "story 071, sentence 064\tamin. has 1 lines, dropping 1\n",
            "story 072, sentence 045\tamin. has 1 lines, dropping 1\n",
            "story 074, sentence 070\tamin. has 1 lines, dropping 1\n",
            "story 075, sentence 075\tamin. has 1 lines, dropping 1\n",
            "story 095, sentence 008\t?aming. has 1 lines, dropping 1\n",
            "story 096, sentence 049\t?aming. has 1 lines, dropping 1\n",
            "story 097, sentence 022\tamin. has 1 lines, dropping 1\n"
          ]
        }
      ],
      "source": [
        "stories = {} # nested dict holding content of stories mapped to sid\n",
        "i = 0 # iterate through raw_lines\n",
        "\n",
        "while i < len(raw_lines):\n",
        "    if not is_header(raw_lines[i]):\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    # split on any whitespace once --> sid is the 3 digit number and title is rest of line\n",
        "    sid, title = raw_lines[i].split(None, 1)\n",
        "    \n",
        "    # subtitle is the next line\n",
        "    subtitle = raw_lines[i+1]\n",
        "\n",
        "    # skip 2 lines to first sentence\n",
        "    i += 2\n",
        "\n",
        "    # collect all lines for current story\n",
        "    story_lines = []\n",
        "    while i < len(raw_lines) and not is_header(raw_lines[i]):\n",
        "        story_lines.append(raw_lines[i])\n",
        "        i += 1\n",
        "\n",
        "    # remove compiled english translation at the end of every story\n",
        "    # find every index in story_lines where a sentence starts\n",
        "    starts = []\n",
        "    for idx, line in enumerate(story_lines):\n",
        "        if is_sentence_start(line):\n",
        "            starts.append(idx)\n",
        "\n",
        "    if starts:\n",
        "        last_start = starts[-1] # where last sentence begins\n",
        "        # scan from that last_start forward to find the free‐translation line\n",
        "        for j in range(last_start, len(story_lines)):\n",
        "            if '\\t' not in story_lines[j]:\n",
        "                # keep everything up to and including that line\n",
        "                story_lines = story_lines[:j+1]\n",
        "                break\n",
        "\n",
        "    # list for the parsed sentences\n",
        "    sentences = []\n",
        "\n",
        "    # process each sentence block\n",
        "    for s_idx in range(len(starts)):\n",
        "        start_idx = starts[s_idx]\n",
        "        if s_idx + 1 < len(starts):\n",
        "            end_idx = starts[s_idx + 1]\n",
        "        else:\n",
        "            end_idx = len(story_lines)\n",
        "        block = story_lines[start_idx:end_idx]\n",
        "\n",
        "        # remove any footnotes (starts with open bracket)\n",
        "        filtered_block = []\n",
        "        for line in block:\n",
        "            if not line.startswith('['):\n",
        "                filtered_block.append(line)\n",
        "\n",
        "        block = filtered_block\n",
        "        if len(block) == 0:\n",
        "            continue\n",
        "\n",
        "        free_tr = block[-1]\n",
        "        tier_lines = block[:-1]\n",
        "\n",
        "        # TODO: lines containing one word are being counted as the free translation because they contain no tabs\n",
        "            # lines dropped --> change these lines during QC\n",
        "\n",
        "        # check that each block has 3 tiers (orig, morph, gloss), if not then drop lines\n",
        "        total = len(tier_lines)\n",
        "        triplets = total // 3\n",
        "        if triplets * 3 != total:\n",
        "            print(f\"story {sid}, sentence {tier_lines[0]} has {total} lines, dropping {total-triplets*3}\")\n",
        "        tier_lines = tier_lines[:triplets*3]\n",
        "\n",
        "        # chunk into orig, morph, gloss\n",
        "        orig_parts, morph_parts, gloss_parts = [], [], []\n",
        "        for k in range(0, len(tier_lines), 3):\n",
        "            orig_line = tier_lines[k]\n",
        "            orig_line_clean = re.sub(r'^\\d{3}\\t+', '', orig_line)  # remove sentence number and tab\n",
        "            orig_parts.append(orig_line_clean)\n",
        "            morph_parts.append(tier_lines[k+1])\n",
        "            gloss_parts.append(tier_lines[k+2])\n",
        "\n",
        "        # join sentences that wrapped more than once\n",
        "        full_orig = ' '.join(orig_parts)\n",
        "        full_morph = ' '.join(morph_parts)\n",
        "        full_gloss = ' '.join(gloss_parts)\n",
        "\n",
        "        sentences.append({\n",
        "            'orig': full_orig,\n",
        "            'morph': full_morph,\n",
        "            'gloss': full_gloss,\n",
        "            'free_tr': free_tr\n",
        "        })\n",
        "\n",
        "    stories[sid] = {\n",
        "        'title':     title,\n",
        "        'subtitle':  subtitle,\n",
        "        'sentences': sentences\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8c3f8881",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check story 002, sentence 3, word 8\n",
            "check story 078, sentence 4, word 19\n",
            "wrote 100 XML files to ./xml_output/\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('xml_output', exist_ok=True)\n",
        "for sid, data in stories.items():\n",
        "    # <TEXT>\n",
        "    TEXT = etree.Element('TEXT')\n",
        "    TEXT.set('id', f\"PaiwanCh2_{sid}_{data['title']}\")\n",
        "    TEXT.set('{http://www.w3.org/XML/1998/namespace}lang', 'pwn')\n",
        "    TEXT.set('copyright', 'public domain')\n",
        "    TEXT.set('citation',\n",
        "                'Early, R. J., & Whitehorn, J. (2003). One hundred Paiwan texts. Pacific Linguistics, Research School of Pacific and Asian Studies, The Australian National University.')\n",
        "    TEXT.set('BibTeX_citation',\n",
        "                '@book{100paiwantexts, author = {Robert Early and John Whitehorn}, title = {One Hundred Paiwan Texts}, year = {2003}, publisher = {Pacific Linguistics, Research School of Pacific and Asian Studies, The Australian National University}}')\n",
        "    TEXT.set('source', data['subtitle'])\n",
        "\n",
        "    ## dialect attribute\n",
        "    dialect = \"Unknown\"\n",
        "    subtitle = data['subtitle']\n",
        "    for village_name, region in dialect_map.items():\n",
        "        if village_name in subtitle:\n",
        "            dialect = region\n",
        "            break\n",
        "    if dialect != \"Unknown\":\n",
        "        TEXT.set('dialect', dialect)\n",
        "\n",
        "    # sentences\n",
        "    for sidx, sent in enumerate(data['sentences'], start=1):\n",
        "        S = etree.SubElement(TEXT, 'S', id=f'{sid}S{sidx}')\n",
        "        # FORM original\n",
        "        fo = etree.SubElement(S, 'FORM', kindOf='original')\n",
        "        fo.text = sent['orig']\n",
        "\n",
        "        # free translation\n",
        "        tr = etree.SubElement(S, 'TRANSL')\n",
        "        tr.set('{http://www.w3.org/XML/1998/namespace}lang', 'en')\n",
        "        tr.text = sent['free_tr']\n",
        "\n",
        "        # word‐level breakdown\n",
        "        delims = r'\\t| '\n",
        "        orig_toks = re.split(delims, sent['orig'])\n",
        "        morph_toks = re.split(delims, sent['morph'])\n",
        "        gloss_toks = re.split(delims, sent['gloss'])\n",
        "\n",
        "        for widx, (wform, mform, gform) in enumerate(zip(orig_toks, morph_toks, gloss_toks), start=1):\n",
        "            W = etree.SubElement(S, 'W', id=f'{sid}S{sidx}W{widx}')\n",
        "            # word FORM\n",
        "            wo = etree.SubElement(W, 'FORM')\n",
        "            wo.text = wform\n",
        "            # split the morph and gloss on hyphens\n",
        "            morph_pieces = mform.split('-')\n",
        "            gloss_pieces = gform.split('-')\n",
        "\n",
        "            # if they match up, create one <M> per morpheme\n",
        "            if len(morph_pieces) == len(gloss_pieces):\n",
        "                for mid, (mp, gp) in enumerate(zip(morph_pieces, gloss_pieces)):\n",
        "                    M = etree.SubElement(W, 'M', id=f'{sid}S{sidx}W{widx}M{mid}')\n",
        "                    mo = etree.SubElement(M, 'FORM', kindOf='original')\n",
        "                    mo.text = mp\n",
        "                    mg = etree.SubElement(M, 'TRANSL')\n",
        "                    mg.text = gp\n",
        "            else:\n",
        "                # if counts don't match, keep together\n",
        "                # TODO: check this during QC\n",
        "                print(f\"check story {sid}, sentence {sidx}, word {widx}\")\n",
        "                M = etree.SubElement(W, 'M', id=f'{sid}S{sidx}W{widx}M{mid}')\n",
        "                mo = etree.SubElement(M, 'FORM', kindOf='original')\n",
        "                mo.text = mform\n",
        "                mg = etree.SubElement(M, 'TRANSL')\n",
        "                mg.text = gform\n",
        "\n",
        "\n",
        "    tree = etree.ElementTree(TEXT)\n",
        "    output = f'xml_output/PaiwanCh2_{sid}.xml'\n",
        "    tree.write(output, encoding='UTF-8', pretty_print=True)\n",
        "\n",
        "print(f\"wrote {len(stories)} XML files to ./xml_output/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
